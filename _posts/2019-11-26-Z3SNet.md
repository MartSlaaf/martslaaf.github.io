---
title: "Zero-Shot Semantic Segmentation (Bucher et al. 2019)"
date: 2019-11-26T13:00:00-02:00
categories:
  - article-essence
tags:
  - Segmentation
---

Authors of the paper aim to achieve high segmentation scores on for the classes, which have no train markup, but the images containing these classes being accessible.
The approach uses semantic relations between words used as labels, to estimate the relations between the expected latent representations.

Suppose we have the dataset composed of two parts: 
 - $$ \{x_k, y_k\}_{k=1}^{N} $$, containing the segmentation targets $$ y $$ for the classes $$ \mathcal{C} $$
 - $$ \{\tilde{x}_k, \tilde{y}_k\}_{k=1}^{L} $$, containing only classification targets $$ \tilde{y} $$ for the superset of classes $$ \mathcal{\tilde{C}}: \vert \mathcal{C} \vert < \vert \mathcal{\tilde{C}} \vert $$

The additional requirement is, we have the class labels described with real words.
Thus, we can use pre-trained model $$ a(y_k) $$ which maps a class label to the vector of the fixed length. 
We will use this model to exploit the semantics of representations behind these words.
It could be trained embedding or any other approach which provides the semantically-consistent mapping.

The training procedure is three-step:

*Classical FCN training*. 
On this step we simply train FCN $$f_{\theta}$$ segmentation network in a classical, fully-supervised manner on the first part of dataset $$ \{x_k, y_k\}_{k=1}^{N} $$.
Suppose, we split the FCN to the two parts, feature generator and classifier. 
Applied one-by-one these two networks will give the same result $$f_{\theta_2} \left( f_{\theta_1} \left( x_k \right) \right) =  f_{\theta} \left(x_k \right)$$.
For the sake of simplicity, we can suppose the $$ f_{\theta_2} $$ is the last layer converting latent features to the class scores but generally this is not required.

*Generative model training*.
Now we compose the new dataset, which will map the latent representation of a pixel in the segmentation network to the embedded class representation of this pixel.
So, the new dataset is $$ \{ a(y_{k,i,j}), f_{\theta_2}(x_k)_{i, j} \}_{k,i,j = 1}^{N, W, H} $$.
Now we train generative model $$ f_{\sigma}\left(a\left(y_{k,i,j}\right)\right) \rightarrow f_{\theta_2}(x_k)_{i,j} $$.
We require from this model to match distributions of generated and real features for each class from the $$ \mathcal{C} $$.
So, now we have the generator of latent pixel features conditioned by their class of this pixel.
If our generative model and embeddings are generalized well, we can use them to generate latent pixel features of the unseen classes.

*Classificator retraining*.
Now we can generate the dataset of feature-class pairs $$ \{f_{\sigma} \left( a \left( c_i \right) \right), c_i \}_{i=1}^K $$ for each class $$ c_i \in \mathcal{C} \cup \mathcal{\tilde{C}} $$ of an arbitrary size $$ K $$.
We create new network $$ f_{\hat{\theta}_2} $$ instead of $$ f_{\theta_2} $$ having the same input size, and expanded output of size $$ \vert \mathcal{C} \cup \mathcal{\tilde{C}} \vert $$, and train it on the generated data.
After training we re-assemble FCN as $$ f_{theta} (x) := f_{\hat{\theta}_2} \left(  f_{\theta_1}\left(x\right)\right) $$.

Now, if we have well-trained all the components, we can use our new network for the segmentation of classes, which we have not segmentation markup for.
Authors show good results both qualitatively and quantitatively.
There are additional tricks to achieve more quality, including post-processing and self-training.

I suppose, that the main requirement to apply this approach in a real setting is having well-established labels for classes of interest.
Also, I suppose, it works better, if you have the closely corresponded classes in the $$ \mathcal{C} $$ and $$ \mathcal{\tilde{C}} $$ (e.g. if network haven't seen vehicles on the train time, it will be difficult to distinguish between sedan and truck).

[paper](https://arxiv.org/abs/1906.00817)\\
[github](https://github.com/valeoai/ZS3)
